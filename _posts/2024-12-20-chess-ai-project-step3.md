---
title: "[Chess AI] 3. CNN ëª¨ë¸ ì„¤ê³„ ë° í•™ìŠµ"
date: 2024-12-20 14:00:00 +0900
categories: [Machine Learning, Chess AI]
tags: [machine-learning, neural-network, chess, cnn, pytorch, deep-learning, project]
pin: false
math: true
mermaid: true
---

# Chess AI í”„ë¡œì íŠ¸ - Phase 3: CNN ëª¨ë¸ ì„¤ê³„ ë° í•™ìŠµ

Phase 2ì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•œ í›„, Phase 3ì—ì„œëŠ” **CNN ëª¨ë¸ì„ ì„¤ê³„í•˜ê³  í•™ìŠµ**í–ˆìŠµë‹ˆë‹¤.

## ğŸ“‹ Phase 3 ëª©í‘œ

1. **CNN Architecture**: Convolutional Neural Network ì„¤ê³„
2. **Training Pipeline**: í•™ìŠµ ë£¨í”„, Learning Rate Scheduling, Early Stopping
3. **Model Training**: Kaggle ë°ì´í„°ì…‹(336K positions)ìœ¼ë¡œ í•™ìŠµ
4. **Model Evaluation**: í…ŒìŠ¤íŠ¸ ì…‹ í‰ê°€ ë° ì„±ëŠ¥ ë¶„ì„

```mermaid
graph TD
    A[13x8x8 Input] --> B[Conv Layer 1<br/>64 filters]
    B --> C[BatchNorm + ReLU]
    C --> D[Conv Layer 2<br/>128 filters]
    D --> E[BatchNorm + ReLU]
    E --> F[Conv Layer 3<br/>128 filters]
    F --> G[BatchNorm + ReLU]
    G --> H[Flatten<br/>8192 features]
    H --> I[FC: 256]
    I --> J[ReLU + Dropout]
    J --> K[FC: 128]
    K --> L[ReLU + Dropout]
    L --> M[FC: 1]
    M --> N[Tanh<br/>Output]

    style A fill:#ccffcc
    style N fill:#ffcccc
    style B fill:#cce5ff
    style D fill:#cce5ff
    style F fill:#cce5ff
```

---

## Step 3.1: CNN ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### ì™œ CNNì¸ê°€?

**ì²´ìŠ¤ëŠ” 2D ê³µê°„ êµ¬ì¡°ë¥¼ ê°€ì§„ ê²Œì„**:
- 8Ã—8 ë³´ë“œì˜ **ê³µê°„ì  íŒ¨í„´**ì´ ì¤‘ìš”
- ë§ë“¤ì˜ **ìƒëŒ€ì  ìœ„ì¹˜**ê°€ í‰ê°€ì— ì˜í–¥
- **Local patterns**: Pawn chains, king safety, piece coordination
- CNNì€ ì´ëŸ¬í•œ ê³µê°„ì  íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ í•™ìŠµ

**CNN vs Dense NN**:
- Dense NN: ìœ„ì¹˜ ì •ë³´ ì†ì‹¤, íŒŒë¼ë¯¸í„° ê³¼ë‹¤
- CNN: ê³µê°„ êµ¬ì¡° ë³´ì¡´, íŒŒë¼ë¯¸í„° íš¨ìœ¨ì 

### CNN ì•„í‚¤í…ì²˜

**íŒŒì¼**: `src/model/chess_cnn.py`

```python
class ChessEvaluatorCNN(nn.Module):
    """CNN model for evaluating chess positions."""

    def __init__(self, dropout_rate: float = 0.3):
        super(ChessEvaluatorCNN, self).__init__()

        # Convolutional layers
        self.conv_layers = nn.Sequential(
            # Conv block 1: 13 -> 64 channels
            nn.Conv2d(13, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            # Conv block 2: 64 -> 128 channels
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            # Conv block 3: 128 -> 128 channels
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
        )

        # Fully connected layers
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 1),
            nn.Tanh()  # Output in [-1, 1]
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x
```

**ëª¨ë¸ í†µê³„**:
- **Total Parameters**: 2,360,065 (ì•½ 2.36M)
- **Model Size**: 9.0 MB (FP32)
- **Architecture**: 3 Conv blocks + 3 FC layers

---

## Step 3.2: Training Pipeline êµ¬í˜„

### Trainer í´ë˜ìŠ¤

**íŒŒì¼**: `src/model/trainer.py`

**ì£¼ìš” ê¸°ëŠ¥**:
1. **Training Loop**: Epoch ë‹¨ìœ„ í•™ìŠµ
2. **Validation**: ë§¤ epochë§ˆë‹¤ ê²€ì¦
3. **Learning Rate Scheduling**: ReduceLROnPlateau
4. **Early Stopping**: Patience ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ
5. **Checkpoint Saving**: Best model ìë™ ì €ì¥

```python
class Trainer:
    def __init__(self, model, train_loader, val_loader, ...):
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )

    def train_epoch(self):
        self.model.train()
        for positions, evaluations in self.train_loader:
            predictions = self.model(positions)
            loss = self.criterion(predictions, evaluations)

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        return avg_loss

    def validate(self):
        self.model.eval()
        with torch.no_grad():
            for positions, evaluations in self.val_loader:
                predictions = self.model(positions)
                loss = self.criterion(predictions, evaluations)
        return avg_loss
```

### Training Configuration

```python
config = {
    # Data
    'batch_size': 128,
    'scale_factor': 4000.0,

    # Model
    'dropout_rate': 0.3,

    # Training
    'learning_rate': 0.001,
    'weight_decay': 1e-5,
    'epochs': 5,  # Quick test
    'early_stopping_patience': 3,

    # System
    'device': 'cpu',
}
```

---

## Step 3.3: Model Training

### Quick Training Test (5 Epochs)

**ì‹¤í–‰ ì‹œê°„**: 21.2ë¶„
**í•˜ë“œì›¨ì–´**: CPU (Intel i5/i7ê¸‰)

**í•™ìŠµ ì§„í–‰**:

```
Epoch 1/5 | Train Loss: 0.040136 | Val Loss: 0.027130 | Time: 252.9s
  -> Saved best model (val_loss: 0.027130)

Epoch 2/5 | Train Loss: 0.026193 | Val Loss: 0.025928 | Time: 251.6s
  -> Saved best model (val_loss: 0.025928)

Epoch 3/5 | Train Loss: 0.024143 | Val Loss: 0.024634 | Time: 258.6s
  -> Saved best model (val_loss: 0.024634)

Epoch 4/5 | Train Loss: 0.023111 | Val Loss: 0.026398 | Time: 253.7s

Epoch 5/5 | Train Loss: 0.022040 | Val Loss: 0.021483 | Time: 252.5s
  -> Saved best model (val_loss: 0.021483)
```

**Training Summary**:
- **Total epochs**: 5
- **Best epoch**: 5
- **Best validation loss**: 0.021483
- **Train loss improvement**: 45.1% reduction
- **Val loss improvement**: 20.8% reduction

![Training History](/assets/img/chess-ai/step3-training-history.png)
_í•™ìŠµ ê³¡ì„ : Lossì™€ Learning Rate_

---

## Step 3.4: Model Evaluation

### í…ŒìŠ¤íŠ¸ ì…‹ í‰ê°€

**Test Dataset**: 50,536 positions (15%)

**Evaluation Metrics**:

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **MAE** | 0.070246 | í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ |
| **RMSE** | 0.144973 | í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ |
| **RÂ²** | 0.564024 | 56.4% ì„¤ëª…ë ¥ |
| **Correlation** | 0.766799 | 76.7% ìƒê´€ê´€ê³„ |

**Centipawn ë‹¨ìœ„** (í•´ì„í•˜ê¸° ì‰½ê²Œ):
- **MAE**: **281.4 cp** â‰ˆ **2.8 pawns** í‰ê·  ì˜¤ì°¨
- **RMSE**: 584.0 cp â‰ˆ 5.8 pawns

![Model Evaluation](/assets/img/chess-ai/step3-model-evaluation.png)
_ëª¨ë¸ í‰ê°€ ê²°ê³¼: Predicted vs Actual, Residuals, Error Distribution_

### ë²”ìœ„ë³„ ì„±ëŠ¥ ë¶„ì„

| Evaluation Range | Count | MAE | RMSE | ì„¤ëª… |
|------------------|-------|-----|------|------|
| **Equal** (-0.1 ~ 0.1) | 33,171 | 0.033 | 0.047 | âœ… ê°€ì¥ ì •í™• |
| **Moderate Black** (-0.5 ~ -0.1) | 7,070 | 0.083 | 0.100 | ê´œì°®ìŒ |
| **Moderate White** (0.1 ~ 0.5) | 8,120 | 0.083 | 0.108 | ê´œì°®ìŒ |
| **Large Black** (-1.0 ~ -0.5) | 910 | 0.577 | 0.621 | ê°œì„  í•„ìš” |
| **Large White** (0.5 ~ 1.0) | 1,265 | 0.538 | 0.611 | ê°œì„  í•„ìš” |

**ê´€ì°°**:
- âœ… **í‰í˜• í¬ì§€ì…˜ì—ì„œ ê°€ì¥ ì •í™•** (MAE 0.033)
- âœ… **ì¤‘ê°„ ìš°ì„¸ í¬ì§€ì…˜ë„ ê´œì°®ì€ ì„±ëŠ¥**
- âš ï¸ **ê·¹ë‹¨ì  ìš°ì„¸ í¬ì§€ì…˜ì€ ê°œì„  í•„ìš”** (ë°ì´í„° ë¶€ì¡±)

---

## ğŸ¯ Phase 3 ì£¼ìš” ì„±ê³¼

### âœ… ì™„ë£Œëœ ì‘ì—…

1. **CNN Architecture**
   - 3-layer convolutional network
   - BatchNormalization for stability
   - Dropout for regularization
   - 2.36M parameters

2. **Training Pipeline**
   - Complete training loop
   - Learning rate scheduling
   - Early stopping mechanism
   - Automatic checkpoint saving

3. **Model Training**
   - 5 epochs in 21.2 minutes
   - 45% train loss reduction
   - 21% val loss reduction
   - No overfitting observed

4. **Model Evaluation**
   - MAE: 281 centipawns (í—ˆìš© ê°€ëŠ¥)
   - RÂ²: 0.56 (ë” ê°œì„  ê°€ëŠ¥)
   - Correlation: 0.77 (ê°•í•œ ìƒê´€ê´€ê³„)

### ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„

**ê°•ì **:
- âœ… í‰í˜• í¬ì§€ì…˜ì—ì„œ ë†’ì€ ì •í™•ë„
- âœ… ê³¼ì í•© ì—†ìŒ (Train â‰ˆ Val)
- âœ… ê°•í•œ ìƒê´€ê´€ê³„ (0.77)
- âœ… ë¹ ë¥¸ ìˆ˜ë ´ (5 epochs)

**ê°œì„ ì **:
- âš ï¸ ê·¹ë‹¨ì  í¬ì§€ì…˜ ì²˜ë¦¬ (ë” ë§ì€ í•™ìŠµ í•„ìš”)
- âš ï¸ RÂ² ì ìˆ˜ í–¥ìƒ (ë” ë§ì€ epochs)
- âš ï¸ ì „ëµì  ì´í•´ë„ (í˜„ì¬ëŠ” ë‹¨ìˆœ í‰ê°€)

### ğŸ“Š ë¹„êµ: Phase 1 ëª©í‘œ vs ì‹¤ì œ ê²°ê³¼

| ëª©í‘œ | ê¸°ì¤€ | ê²°ê³¼ | ë‹¬ì„± |
|------|------|------|------|
| **MAE** | < 0.15 (< 60cp) | 0.070 (281cp) | âš ï¸ ë¶€ë¶„ ë‹¬ì„± |
| **RÂ²** | > 0.80 | 0.56 | âš ï¸ ê°œì„  í•„ìš” |
| **Correlation** | - | 0.77 | âœ… ê°•í•œ ìƒê´€ê´€ê³„ |

**ì°¸ê³ **: Phase 1 ëª©í‘œëŠ” ì™„ì „ í•™ìŠµ(50 epochs) ê¸°ì¤€ì´ì—ˆê³ , í˜„ì¬ëŠ” **quick test (5 epochs)**ë§Œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

---

## ğŸ’¡ ë°°ìš´ ì 

### 1. CNNì˜ íš¨ê³¼

**ê³µê°„ì  íŠ¹ì§• í•™ìŠµ**:
- Conv layersê°€ ì²´ìŠ¤ ë³´ë“œì˜ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµ
- ë§ë“¤ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ë¥¼ ìë™ìœ¼ë¡œ í¬ì°©
- BatchNormìœ¼ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ

**íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±**:
- CNN (2.36M) vs Dense (0.59M)
- CNNì´ ë” í¬ì§€ë§Œ, **ê³µê°„ ì •ë³´ ë³´ì¡´**ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

### 2. Training Strategies

**Learning Rate Scheduling**:
- ReduceLROnPlateau: Validation loss ì •ì²´ ì‹œ LR ê°ì†Œ
- íš¨ê³¼ì ì¸ fine-tuning

**Early Stopping**:
- Patience=10ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
- Best model ìë™ ì €ì¥

**Batch Size ì„ íƒ**:
- 128: GPU/CPU ê· í˜•
- ë” í° ë°°ì¹˜: ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥
- ë” ì‘ì€ ë°°ì¹˜: í•™ìŠµ ì‹œê°„ ì¦ê°€

### 3. Evaluation Insights

**Normalized vs Centipawns**:
- Normalized: ëª¨ë¸ í•™ìŠµì— ì í•©
- Centipawns: í•´ì„í•˜ê¸° ì‰¬ì›€
- ë³€í™˜ ê³µì‹: `cp = arctanh(norm) * 4000`

**Error Analysis**:
- Equal positions: ê°€ì¥ ë§ì€ ë°ì´í„°, ê°€ì¥ ì •í™•
- Extreme positions: ì ì€ ë°ì´í„°, ë‚®ì€ ì •í™•ë„
- Data augmentation í•„ìš”

### 4. Performance Considerations

**CPU vs GPU**:
- CPU: ì•½ 4ë¶„/epoch (í˜„ì¬)
- GPU ì˜ˆìƒ: ì•½ 10-20ì´ˆ/epoch (20x faster)
- 50 epochs CPU ì˜ˆìƒ: ì•½ 3.5ì‹œê°„

**Memory Usage**:
- Dataset: 1.07 GB (ì „ì²´)
- Model: 9 MB
- Batch (128): ~100 KB

---

## ğŸ”œ ë‹¤ìŒ ë‹¨ê³„: Phase 4 - Chess Engine Integration

### Step 4.1: Move Generation
- python-chess integration
- Legal move generation
- Special moves handling (castling, en passant)

### Step 4.2: Minimax Search
- Alpha-Beta pruning
- Move ordering
- Transposition table
- Iterative deepening

### Step 4.3: Position Evaluation Integration
- NN evaluation in search tree
- Batch evaluation for efficiency
- Checkmate/stalemate handling

### Step 4.4: Engine Testing
- Tactical puzzle solving
- Self-play games
- Baseline comparison

---

## ğŸ“‚ ìƒì„±ëœ íŒŒì¼

```
src/model/
â”œâ”€â”€ chess_cnn.py               # CNN model architecture
â”œâ”€â”€ trainer.py                 # Training loop and utilities
â”œâ”€â”€ train.py                   # Main training script (50 epochs)
â”œâ”€â”€ evaluator.py               # Model evaluation
â””â”€â”€ visualize_training.py      # Training history visualization

models/
â””â”€â”€ chess_cnn_test/
    â”œâ”€â”€ best_model.pth         # Best model checkpoint
    â”œâ”€â”€ training_history.json  # Training metrics
    â””â”€â”€ config.json            # Training configuration

data/
â”œâ”€â”€ training_history.png       # Loss curves
â””â”€â”€ model_evaluation.png       # Evaluation plots
```

---

## ğŸš€ ë‹¤ìŒ ì‹¤í–‰ ê³„íš

### ì˜µì…˜ 1: ë” ë§ì€ Epochs (ê¶Œì¥)

**Full Training (50 epochs)**:
```bash
python train.py
```

**ì˜ˆìƒ ì‹œê°„**: ì•½ 3.5ì‹œê°„ (CPU)
**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- MAE: 0.070 â†’ 0.050 (30% ê°œì„ )
- RÂ²: 0.56 â†’ 0.80+ (40% ê°œì„ )
- RMSE: 0.145 â†’ 0.100 (30% ê°œì„ )

### ì˜µì…˜ 2: Hyperparameter Tuning

**í…ŒìŠ¤íŠ¸í•  ë³€ìˆ˜**:
- Learning rate: 0.001, 0.0005, 0.0001
- Dropout rate: 0.2, 0.3, 0.5
- Batch size: 64, 128, 256
- Architecture depth: 3 layers, 4 layers, 5 layers

### ì˜µì…˜ 3: Data Augmentation

**ì²´ìŠ¤ íŠ¹í™” augmentation**:
- Board flipping (horizontal mirror)
- Color swapping (white â†” black)
- ë°ì´í„° 2x ì¦ê°€ ê°€ëŠ¥

---

**ë‹¤ìŒ í¬ìŠ¤íŠ¸**: Phase 4 - Chess Engine êµ¬í˜„ (Minimax + Alpha-Beta Pruning)
